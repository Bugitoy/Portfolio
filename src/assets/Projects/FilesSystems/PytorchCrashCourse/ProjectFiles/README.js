export const PytorchCrashCourseFile = {
    'README.md': {
      type: 'file',
      title: 'README.md',
      lines: [
        { code: 'Give Me 1 Hour, You Will Build and Train a Neural Network From Scratch' },
        { code: '' },
        { code: 'Intro' },
        { code: '' },
        { code: 'You\'ve seen PyTorch code. Maybe in a research paper or a tutorial. You saw model.train(), loss.backward(), and optimizer.step().' },
        { code: 'You wondered what they actually do.' },
        { code: 'Maybe you tried to read the documentation. Got hit with a wall of classes. nn.Module, torch.optim, computation graphs. It felt like opaque, magical black boxes.' },
        { code: '' },
        { code: 'Here\'s the truth: They\'re not black boxes. They are elegant wrappers for the linear algebra and calculus you already know.' },
        { code: '' },
        { code: 'The entire deep learning training process, from the simplest model to a giant LLM, is just a five-step algorithm. That\'s it. Everything else is just an implementation detail.' },
        { code: '' },
        { code: '- The nn.Linear layer? It\'s just X @ W + b.' },
        { code: '- The loss function? Just a formula to measure error, like torch.mean((y - y_hat)**2).' },
        { code: '- The magical loss.backward()? It\'s just your old friend, the chain rule (backpropagation), automated.' },
        { code: '- The optimizer.step()? It\'s just the gradient descent update rule: W -= learning_rate * W.grad.' },
        { code: '' },
        { code: 'In this guide, we\'ll implement the raw math first, then use PyTorch\'s professional tools.' },
        { code: 'You\'ll see why the tools exist by first doing the work manually. Once you\'ve implemented the raw math, the professional code becomes obvious—just a clean, powerful way to organize what you\'ve already built.' },
        { code: '' },
        { code: 'And it all boils down to these 5 steps.' },
        { code: 'Master this five-step loop, and you can understand the training code for any model, from a simple regression to a state-of-the-art Large Language Model.' },
        { code: '' },
        { code: '1. Prediction: Make a prediction y_hat = f(X, theta) | y_hat = X @ W + b | y_hat = model(X)' },
        { code: '2. Loss Calc: Quantify the error L = Loss(y_hat, y) | loss = torch.mean((y_hat - y)**2) | loss = criterion(y_hat, y)' },
        { code: '3. Gradient Calc: Find the slope of the loss | loss.backward() | loss.backward()' },
        { code: '4. Param Update: Step down the slope | W -= lr * W.grad | optimizer.step()' },
        { code: '5. Gradient Reset: Reset for the next loop | W.grad.zero_() | optimizer.zero_grad()' },
        { code: '' },
        { code: 'Even better—this loop is universal. The model in Step 1 might become a massive Transformer, and the optimizer in Step 4 might be a fancy one like Adam, but the fundamental five-step logic never changes.' },
        { code: '' },
        { code: 'This isn\'t an analogy. The nn.Linear layer used in LLMs is the exact same X @ W + b operation you will build in Part 2. The Feed-Forward Network inside a Transformer is the exact same stack of layers you will build in Part 4. You are learning the literal, fundamental building blocks of modern AI.' },
        { code: '' },
        { code: 'Give me one hour. We\'ll build your understanding from the ground up. Raw math, real code, and a clear path to the professional tools.' },
        { code: 'Ready? Let\'s begin.' },
        { code: '' },
        { code: 'Part 1: The Core Data Structure - torch.Tensor' },
        { code: '' },
        { code: 'Welcome to PyTorch! Everything we do—from loading data to defining complex neural networks—will be built on one core object: the torch.Tensor.' },
        { code: '' },
        { code: 'The Analogy: Think of a Tensor as the fundamental building block, the basic "noun" of the PyTorch language. It\'s a multi-dimensional array, very similar to a NumPy array, but with special powers for deep learning that we will unlock step-by-step.' },
        { code: 'Our goal in this section is to master this single, crucial object: how to create it and how to understand its properties.' },
        { code: '' },
        { code: '1.1. The Three Common Patterns for Creating Tensors' },
        { code: 'There are many ways to create a tensor, but they generally fall into three common patterns you\'ll see everywhere.' },
        { code: '' },
        { code: 'Pattern 1: Direct Creation from Existing Data' },
        { code: 'This is the most straightforward. You have a Python list of numbers, and you want to convert it into a PyTorch tensor using torch.tensor().' },
        { code: '' },
        { code: 'import torch' },
        { code: '' },
        { code: '# Input: A standard Python list' },
        { code: 'data = [[1, 2, 3], [4, 5, 6]]' },
        { code: 'my_tensor = torch.tensor(data)' },
        { code: '' },
        { code: 'print(my_tensor)' },
        { code: '' },
        { code: 'Output:' },
        { code: 'tensor([[1, 2, 3],' },
        { code: '        [4, 5, 6]])' },
        { code: '' },
        { code: 'Pattern 2: Creation from a Desired Shape' },
        { code: 'This is extremely common when building models. You don\'t know the exact values for your weights yet, but you know the dimensions (shape) you need.' },
        { code: '' },
        { code: '- torch.ones() or torch.zeros(): Fills the tensor with 1s or 0s.' },
        { code: '- torch.randn(): Fills the tensor with random numbers from a standard normal distribution. This is the standard way to initialize a model\'s weights.' },
        { code: '' },
        { code: '# Input: A shape tuple (2 rows, 3 columns)' },
        { code: 'shape = (2, 3)' },
        { code: '' },
        { code: '# Create tensors with this shape' },
        { code: 'ones = torch.ones(shape)' },
        { code: 'zeros = torch.zeros(shape)' },
        { code: 'random = torch.randn(shape)' },
        { code: '' },
        { code: 'print(f"Ones Tensor:\\n {ones}\\n")' },
        { code: 'print(f"Zeros Tensor:\\n {zeros}\\n")' },
        { code: 'print(f"Random Tensor:\\n {random}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Ones Tensor:' },
        { code: ' tensor([[1., 1., 1.],' },
        { code: '        [1., 1., 1.]])' },
        { code: '' },
        { code: 'Zeros Tensor:' },
        { code: ' tensor([[0., 0., 0.],' },
        { code: '        [0., 0., 0.]])' },
        { code: '' },
        { code: 'Random Tensor:' },
        { code: ' tensor([[ 0.3815, -0.9388,  1.6793],' },
        { code: '        [-0.3421,  0.5898,  0.3609]])' },
        { code: '' },
        { code: 'Pattern 3: Creation by Mimicking Another Tensor\'s Properties' },
        { code: 'Often, you have an existing tensor and you need to create a new one that has the exact same shape, data type, and is on the same device. Instead of manually copying these properties, PyTorch provides handy _like functions.' },
        { code: '' },
        { code: '# Input: A \'template\' tensor' },
        { code: 'template = torch.tensor([[1, 2], [3, 4]])' },
        { code: '' },
        { code: '# Create new tensors with the same properties as the template' },
        { code: 'ones_like = torch.ones_like(template)' },
        { code: 'rand_like = torch.randn_like(template, dtype=torch.float) # dtype can be overridden' },
        { code: '' },
        { code: 'print(f"Template Tensor:\\n {template}\\n")' },
        { code: 'print(f"Ones_like Tensor:\\n {ones_like}\\n")' },
        { code: 'print(f"Randn_like Tensor:\\n {rand_like}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Template Tensor:' },
        { code: ' tensor([[1, 2],' },
        { code: '        [3, 4]])' },
        { code: '' },
        { code: 'Ones_like Tensor:' },
        { code: ' tensor([[1, 1],' },
        { code: '        [1, 1]])' },
        { code: '' },
        { code: 'Randn_like Tensor:' },
        { code: ' tensor([[-1.1713, -0.2032],' },
        { code: '        [ 0.3391, -0.8267]])' },
        { code: '' },
        { code: '1.2. What\'s Inside a Tensor? Shape, Type, and Device' },
        { code: 'Every tensor has attributes that describe its metadata. The three you will use constantly are .shape, .dtype, and .device.' },
        { code: '' },
        { code: '# Let\'s create a tensor to inspect' },
        { code: 'tensor = torch.randn(2, 3)' },
        { code: '' },
        { code: 'print(f"Shape of tensor: {tensor.shape}")' },
        { code: 'print(f"Datatype of tensor: {tensor.dtype}")' },
        { code: 'print(f"Device tensor is stored on: {tensor.device}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Shape of tensor: torch.Size([2, 3])' },
        { code: 'Datatype of tensor: torch.float32' },
        { code: 'Device tensor is stored on: cpu' },
        { code: '' },
        { code: 'Let\'s break these down:' },
        { code: '- .shape: This is a tuple that describes the dimensions of the tensor. torch.Size([2, 3]) tells us it\'s a 2D tensor with 2 rows and 3 columns. This is the most important attribute for debugging your models. Mismatched shapes are the #1 source of errors in PyTorch.' },
        { code: '- .device: This tells you where the tensor\'s data is physically stored. By default, it\'s on the cpu. If you have a compatible GPU, you can move it there (.to("cuda")) for massive speedups.' },
        { code: '- .dtype: This describes the data type of the numbers inside the tensor. Notice it defaulted to torch.float32. This is not an accident, and it\'s critically important.' },
        { code: '' },
        { code: 'A Quick but Critical Note on dtype' },
        { code: 'Why torch.float32 and not just regular integers (int64)? The answer is gradients.' },
        { code: 'Gradient descent, the engine of deep learning, works by making tiny, continuous adjustments to a model\'s weights. These adjustments are fractional numbers (like -0.0012), which require a floating-point data type. You can\'t nudge a parameter from 3 to 3.001 if your data type only allows whole numbers.' },
        { code: '' },
        { code: 'Key Takeaway:' },
        { code: '- Model parameters (weights and biases) must be a float type (float32 is the standard).' },
        { code: '- Data that represents categories or counts (like word IDs) can be integers (int64).' },
        { code: '' },
        { code: 'Part 2: The Engine of Autograd - requires_grad' },
        { code: '' },
        { code: 'In Part 1, we learned that tensors are containers for numbers. But their real power comes from a system called Autograd, which stands for automatic differentiation. This is PyTorch\'s built-in gradient calculator.' },
        { code: '' },
        { code: 'The Analogy: If a tensor is the "noun" in PyTorch, then Autograd is the "nervous system." It connects all the operations and allows signals (gradients) to flow backward through the system, enabling learning.' },
        { code: 'Our goal here is to understand the one simple switch that activates this nervous system.' },
        { code: '' },
        { code: '2.1. The "Magic Switch": requires_grad=True' },
        { code: 'By default, PyTorch assumes a tensor is just static data. To tell it that a tensor is a learnable parameter (like a model\'s weight or bias), you must set its requires_grad attribute to True.' },
        { code: 'This is the most important setting in all of PyTorch. It tells Autograd: "This is a parameter my model will learn. From now on, track every single operation that happens to it."' },
        { code: '' },
        { code: '# A standard data tensor (we don\'t need to calculate gradients for our input data)' },
        { code: 'x_data = torch.tensor([[1., 2.], [3., 4.]])' },
        { code: 'print(f"Data tensor requires_grad: {x_data.requires_grad}\\n")' },
        { code: '' },
        { code: '# A parameter tensor (we need to learn this, so we need gradients)' },
        { code: 'w = torch.tensor([[1.0], [2.0]], requires_grad=True)' },
        { code: 'print(f"Parameter tensor requires_grad: {w.requires_grad}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Data tensor requires_grad: False' },
        { code: 'Parameter tensor requires_grad: True' },
        { code: '' },
        { code: '2.2. The Computation Graph: How PyTorch Remembers' },
        { code: 'Once you set requires_grad=True, PyTorch starts building a computation graph behind the scenes. Think of it as a history of all the operations. Every time you perform an operation on a tensor that requires gradients, PyTorch adds a new node to this graph.' },
        { code: '' },
        { code: 'Let\'s build a simple one. We\'ll compute z = x * y where y = a + b.' },
        { code: '' },
        { code: '# Three parameter tensors that we want to learn' },
        { code: 'a = torch.tensor(2.0, requires_grad=True)' },
        { code: 'b = torch.tensor(3.0, requires_grad=True)' },
        { code: 'x = torch.tensor(4.0, requires_grad=True)' },
        { code: '' },
        { code: '# First operation: y = a + b' },
        { code: 'y = a + b' },
        { code: '' },
        { code: '# Second operation: z = x * y' },
        { code: 'z = x * y' },
        { code: '' },
        { code: 'print(f"Result of a + b: {y}")' },
        { code: 'print(f"Result of x * y: {z}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Result of a + b: 5.0' },
        { code: 'Result of x * y: 20.0' },
        { code: '' },
        { code: 'The math is simple. But behind the scenes, PyTorch has built a graph connecting a, b, x, y, and z.' },
        { code: '' },
        { code: '2.3. Peeking Under the Hood: The .grad_fn Attribute' },
        { code: 'How can we prove this graph exists? Every tensor that is the result of an operation on a requires_grad tensor will have a special attribute called .grad_fn. This attribute is a "breadcrumb" that points to the function that created it.' },
        { code: '' },
        { code: '# z was created by multiplication' },
        { code: 'print(f"grad_fn for z: {z.grad_fn}")' },
        { code: '' },
        { code: '# y was created by addition' },
        { code: 'print(f"grad_fn for y: {y.grad_fn}")' },
        { code: '' },
        { code: '# a was created by the user, not by an operation, so it has no grad_fn' },
        { code: 'print(f"grad_fn for a: {a.grad_fn}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'grad_fn for z: <MulBackward0 object at 0x...>' },
        { code: 'grad_fn for y: <AddBackward0 object at 0x...>' },
        { code: 'grad_fn for a: None' },
        { code: '' },
        { code: 'This is the tangible proof of the computation graph. PyTorch knows z came from a multiplication (MulBackward0) and y came from an addition (AddBackward0). When we later ask it to compute gradients, it will use this information to trace its way backward through the graph using the chain rule.' },
        { code: '' },
        { code: 'Part 3: Basic Mathematical & Reduction Operations' },
        { code: '' },
        { code: 'We have our Tensor (the noun) and Autograd (the nervous system). Now we need operations (the verbs) to describe the calculations our model will perform. In deep learning, the vast majority of these operations are surprisingly simple: matrix multiplications and aggregations.' },
        { code: '' },
        { code: 'Our goal is to master these core verbs and, most importantly, the critical dim argument that controls how they work.' },
        { code: '' },
        { code: '3.1. Mathematical Operations: * vs. @' },
        { code: 'This is the single most common point of confusion for beginners. PyTorch has two very different kinds of multiplication.' },
        { code: '' },
        { code: '1. Element-wise Multiplication (*)' },
        { code: 'This operation multiplies elements in the same position. It\'s like overlaying two tensors and multiplying the corresponding cells. The tensors must have the same shape.' },
        { code: '' },
        { code: 'a = torch.tensor([[1, 2], [3, 4]])' },
        { code: 'b = torch.tensor([[10, 20], [30, 40]])' },
        { code: '' },
        { code: '# Calculation: [[1*10, 2*20], [3*30, 4*40]]' },
        { code: 'element_wise_product = a * b' },
        { code: '' },
        { code: 'print(f"Tensor a:\\n {a}\\n")' },
        { code: 'print(f"Tensor b:\\n {b}\\n")' },
        { code: 'print(f"Element-wise Product (a * b):\\n {element_wise_product}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Tensor a:' },
        { code: ' tensor([[1, 2],' },
        { code: '        [3, 4]])' },
        { code: '' },
        { code: 'Tensor b:' },
        { code: ' tensor([[10, 20],' },
        { code: '        [30, 40]])' },
        { code: '' },
        { code: 'Element-wise Product (a * b):' },
        { code: ' tensor([[ 10,  40],' },
        { code: '        [ 90, 160]])' },
        { code: '' },
        { code: '2. Matrix Multiplication (@)' },
        { code: 'This is the standard matrix product from linear algebra. It\'s the core operation of every Linear layer in a neural network. For m1 @ m2, the number of columns in m1 must equal the number of rows in m2.' },
        { code: '' },
        { code: 'm1 = torch.tensor([[1, 2, 3], [4, 5, 6]])   # Shape: (2, 3)' },
        { code: 'm2 = torch.tensor([[7, 8], [9, 10], [11, 12]]) # Shape: (3, 2)' },
        { code: '' },
        { code: '# Calculation for the first element: (1*7) + (2*9) + (3*11) = 58' },
        { code: 'matrix_product = m1 @ m2 # Resulting shape: (2, 2)' },
        { code: '' },
        { code: 'print(f"Matrix 1 (shape {m1.shape}):\\n {m1}\\n")' },
        { code: 'print(f"Matrix 2 (shape {m2.shape}):\\n {m2}\\n")' },
        { code: 'print(f"Matrix Product (m1 @ m2):\\n {matrix_product}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Matrix 1 (shape torch.Size([2, 3])):' },
        { code: ' tensor([[1, 2, 3],' },
        { code: '        [4, 5, 6]])' },
        { code: '' },
        { code: 'Matrix 2 (shape torch.Size([3, 2])):' },
        { code: ' tensor([[ 7,  8],' },
        { code: '        [ 9, 10],' },
        { code: '        [11, 12]])' },
        { code: '' },
        { code: 'Matrix Product (m1 @ m2):' },
        { code: ' tensor([[ 58,  64],' },
        { code: '        [139, 154]])' },
        { code: '' },
        { code: '3.2. Reduction Operations' },
        { code: 'A "reduction" is any operation that reduces the number of elements in a tensor, often by aggregating them. Examples include sum(), mean(), max(), and min().' },
        { code: '' },
        { code: 'scores = torch.tensor([[10., 20., 30.], [5., 10., 15.]])' },
        { code: '' },
        { code: '# By default, reductions apply to the entire tensor' },
        { code: 'total_sum = scores.sum()' },
        { code: 'average_score = scores.mean()' },
        { code: '' },
        { code: 'print(f"Scores Tensor:\\n {scores}\\n")' },
        { code: '# Calculation: 10 + 20 + 30 + 5 + 10 + 15 = 90' },
        { code: 'print(f"Total Sum: {total_sum}")' },
        { code: '# Calculation: 90 / 6 = 15' },
        { code: 'print(f"Overall Mean: {average_score}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Scores Tensor:' },
        { code: ' tensor([[10., 20., 30.],' },
        { code: '        [ 5., 10., 15.]])' },
        { code: '' },
        { code: 'Total Sum: 90.0' },
        { code: 'Overall Mean: 15.0' },
        { code: '' },
        { code: '3.3. The dim Argument: The Most Important Detail' },
        { code: 'This is where reductions become powerful. The dim argument tells the function which dimension to collapse.' },
        { code: '' },
        { code: 'A simple rule of thumb for 2D tensors:' },
        { code: '- dim=0: Collapses the rows (operates "vertically").' },
        { code: '- dim=1: Collapses the columns (operates "horizontally").' },
        { code: '' },
        { code: 'scores = torch.tensor([[10., 20., 30.], [5., 10., 15.]])' },
        { code: '' },
        { code: '# To get the sum FOR EACH ASSIGNMENT, we collapse the student dimension (dim=0)' },
        { code: '# Calculation: [10+5, 20+10, 30+15]' },
        { code: 'sum_per_assignment = scores.sum(dim=0)' },
        { code: '' },
        { code: '# To get the sum FOR EACH STUDENT, we collapse the assignment dimension (dim=1)' },
        { code: '# Calculation: [10+20+30, 5+10+15]' },
        { code: 'sum_per_student = scores.sum(dim=1)' },
        { code: '' },
        { code: 'print(f"Original Scores:\\n {scores}\\n")' },
        { code: 'print(f"Sum per assignment (dim=0): {sum_per_assignment}")' },
        { code: 'print(f"Sum per student (dim=1):    {sum_per_student}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Original Scores:' },
        { code: ' tensor([[10., 20., 30.],' },
        { code: '        [ 5., 10., 15.]])' },
        { code: '' },
        { code: 'Sum per assignment (dim=0): tensor([15., 30., 45.])' },
        { code: 'Sum per student (dim=1):    tensor([60., 30.])' },
        { code: '' },
        { code: 'Part 4: Advanced Indexing & Selection Primitives' },
        { code: '' },
        { code: 'If basic math ops are the "verbs" of PyTorch, then indexing primitives are the "adverbs" and "prepositions"—they let you specify which data to act upon with great precision.' },
        { code: 'Our goal is to learn how to select data in increasingly sophisticated ways, moving from uniform block selection to dynamic, per-row lookups.' },
        { code: '' },
        { code: '4.1. Standard Indexing: The Basics' },
        { code: 'This works just like in Python lists or NumPy. It\'s for selecting uniform "blocks" of data, like entire rows or columns.' },
        { code: '' },
        { code: '# A simple, easy-to-read tensor' },
        { code: 'x = torch.tensor([[0, 1, 2, 3],' },
        { code: '                  [4, 5, 6, 7],' },
        { code: '                  [8, 9, 10, 11]])' },
        { code: '' },
        { code: '# Get the second row (at index 1)' },
        { code: 'row_1 = x[1]' },
        { code: 'print(f"Row 1: {row_1}\\n")' },
        { code: '' },
        { code: '# Get the third column (at index 2)' },
        { code: 'col_2 = x[:, 2]' },
        { code: 'print(f"Column 2: {col_2}\\n")' },
        { code: '' },
        { code: '# Get a specific element (row 1, column 3)' },
        { code: 'element_1_3 = x[1, 3]' },
        { code: 'print(f"Element at (1, 3): {element_1_3}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Row 1: tensor([4, 5, 6, 7])' },
        { code: 'Column 2: tensor([ 2,  6, 10])' },
        { code: 'Element at (1, 3): 7' },
        { code: '' },
        { code: '4.2. Boolean Masking: Selection by Condition' },
        { code: 'Instead of using integer indices, you can use a boolean (True/False) tensor to select only the elements where the mask is True.' },
        { code: '' },
        { code: 'x = torch.tensor([[0, 1, 2, 3], [4, 5, 6, 7]])' },
        { code: '' },
        { code: '# Step 1: Create the boolean mask' },
        { code: 'mask = x > 3' },
        { code: 'print(f"The Boolean Mask (x > 3):\\n {mask}\\n")' },
        { code: '' },
        { code: '# Step 2: Apply the mask to the tensor' },
        { code: 'selected_elements = x[mask]' },
        { code: 'print(f"Selected elements: {selected_elements}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'The Boolean Mask (x > 3):' },
        { code: ' tensor([[False, False, False, False],' },
        { code: '        [ True,  True,  True,  True]])' },
        { code: '' },
        { code: 'Selected elements: tensor([4, 5, 6, 7])' },
        { code: '' },
        { code: '4.3. Conditional Creation: torch.where()' },
        { code: 'This is the tensor equivalent of a ternary if/else statement. It creates a new tensor by choosing values from two other tensors based on a condition.' },
        { code: '' },
        { code: 'x = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])' },
        { code: '' },
        { code: '# Create a new tensor: if a value in x is > 4, use -1, otherwise use the original value' },
        { code: 'y = torch.where(x > 4, -1, x)' },
        { code: '' },
        { code: 'print(f"Original Tensor: {x}")' },
        { code: 'print(f"Result of where: {y}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Original Tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])' },
        { code: 'Result of where: tensor([ 0,  1,  2,  3,  4, -1, -1, -1, -1, -1])' },
        { code: '' },
        { code: '4.4. Primitives for Finding the "Best" Items' },
        { code: 'After a model makes a prediction, we often need to find the item with the highest score.' },
        { code: '- torch.argmax(): Finds the index of the single maximum value.' },
        { code: '- torch.topk(): Finds the k largest values and their indices.' },
        { code: '' },
        { code: 'scores = torch.tensor([' },
        { code: '    [10, 0, 5, 20, 1],  # Scores for item 0' },
        { code: '    [1, 30, 2, 5, 0]   # Scores for item 1' },
        { code: '])' },
        { code: '' },
        { code: '# Find the index of the best score for each item' },
        { code: 'best_indices = torch.argmax(scores, dim=1)' },
        { code: '# For row 0, max is 20 at index 3. For row 1, max is 30 at index 1.' },
        { code: 'print(f"Argmax indices: {best_indices}\\n")' },
        { code: '' },
        { code: '# Find the top 3 scores and their indices for each item' },
        { code: 'top_values, top_indices = torch.topk(scores, k=3, dim=1)' },
        { code: 'print(f"Top 3 values:\\n {top_values}\\n")' },
        { code: 'print(f"Top 3 indices:\\n {top_indices}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Argmax indices: tensor([3, 1])' },
        { code: '' },
        { code: 'Top 3 values:' },
        { code: ' tensor([[20, 10,  5],' },
        { code: '        [30,  5,  2]])' },
        { code: '' },
        { code: 'Top 3 indices:' },
        { code: ' tensor([[3, 0, 2],' },
        { code: '        [1, 3, 2]])' },
        { code: '' },
        { code: '4.5. Primitive for Dynamic Lookups: torch.gather()' },
        { code: 'This is the most advanced and powerful selection tool. The Problem: Standard indexing is for uniform selection. But what if you need to select a different column for each row?' },
        { code: 'The Solution: torch.gather() is purpose-built for this "dynamic lookup." You provide an index tensor that acts as a personalized list of which element to grab from each row.' },
        { code: '' },
        { code: 'data = torch.tensor([' },
        { code: '    [10, 11, 12, 13],  # row 0' },
        { code: '    [20, 21, 22, 23],  # row 1' },
        { code: '    [30, 31, 32, 33]   # row 2' },
        { code: '])' },
        { code: '' },
        { code: '# Our "personalized list" of column indices to select from each row' },
        { code: 'indices_to_select = torch.tensor([[2], [0], [3]])' },
        { code: '' },
        { code: '# Gather from `data` along dim=1 (the column dimension)' },
        { code: 'selected_values = torch.gather(data, dim=1, index=indices_to_select)' },
        { code: '' },
        { code: 'print(f"Selected Values:\\n {selected_values}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Selected Values:' },
        { code: ' tensor([[12],  # From row 0, we gathered the element at index 2' },
        { code: '        [20],  # From row 1, we gathered the element at index 0' },
        { code: '        [33]]) # From row 2, we gathered the element at index 3' },
        { code: '' },
        { code: 'Part 5: The Forward Pass - Manually Making a Prediction' },
        { code: '' },
        { code: 'The "Forward Pass" is the process of taking input data and passing it forward through the model\'s layers to get an output, or prediction.' },
        { code: '' },
        { code: '5.1. The Model: Simple Linear Regression' },
        { code: 'Our model\'s job is to learn the relationship between one input variable (x) and one output variable (y). The formula from linear algebra is our blueprint: y_hat = XW + b' },
        { code: '' },
        { code: '5.2. The Setup: Creating Our Data' },
        { code: '' },
        { code: '# We\'ll create a "batch" of 10 data points' },
        { code: 'N = 10' },
        { code: 'D_in = 1' },
        { code: 'D_out = 1' },
        { code: '' },
        { code: '# Create our input data X' },
        { code: 'X = torch.randn(N, D_in)' },
        { code: '' },
        { code: '# Create our true target labels y by applying the "true" function' },
        { code: 'true_W = torch.tensor([[2.0]])' },
        { code: 'true_b = torch.tensor(1.0)' },
        { code: 'y_true = X @ true_W + true_b + torch.randn(N, D_out) * 0.1 # Add a little noise' },
        { code: '' },
        { code: 'print(f"Input Data X (first 3 rows):\\n {X[:3]}\\n")' },
        { code: 'print(f"True Labels y_true (first 3 rows):\\n {y_true[:3]}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Input Data X (first 3 rows):' },
        { code: ' tensor([[-0.5186],' },
        { code: '        [-0.2582],' },
        { code: '        [-0.3378]])' },
        { code: '' },
        { code: 'True Labels y_true (first 3 rows):' },
        { code: ' tensor([[-0.1030],' },
        { code: '        [ 0.4491],' },
        { code: '        [ 0.3340]])' },
        { code: '' },
        { code: '5.3. The Parameters: The Model\'s "Brain"' },
        { code: 'Now, we create the parameters W and b that our model will learn. We initialize them with random values. Most importantly, we set requires_grad=True.' },
        { code: '' },
        { code: '# Initialize our parameters with random values' },
        { code: 'W = torch.randn(D_in, D_out, requires_grad=True)' },
        { code: 'b = torch.randn(1, requires_grad=True)' },
        { code: '' },
        { code: 'print(f"Initial Weight W:\\n {W}\\n")' },
        { code: 'print(f"Initial Bias b:\\n {b}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Initial Weight W:' },
        { code: ' tensor([[0.4137]], requires_grad=True)' },
        { code: '' },
        { code: 'Initial Bias b:' },
        { code: ' tensor([0.2882], requires_grad=True)' },
        { code: '' },
        { code: '5.4. The Implementation: From Math to Code' },
        { code: '' },
        { code: '# Perform the forward pass to get our first prediction' },
        { code: 'y_hat = X @ W + b' },
        { code: '' },
        { code: 'print(f"Shape of our prediction y_hat: {y_hat.shape}\\n")' },
        { code: 'print(f"Prediction y_hat (first 3 rows):\\n {y_hat[:3]}\\n")' },
        { code: 'print(f"True Labels y_true (first 3 rows):\\n {y_true[:3]}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Shape of our prediction y_hat: torch.Size([10, 1])' },
        { code: '' },
        { code: 'Prediction y_hat (first 3 rows):' },
        { code: ' tensor([[ 0.0737],' },
        { code: '        [ 0.1812],' },
        { code: '        [ 0.1485]], grad_fn=<AddBackward0>)' },
        { code: '' },
        { code: 'True Labels y_true (first 3 rows):' },
        { code: ' tensor([[-0.1030],' },
        { code: '        [ 0.4491],' },
        { code: '        [ 0.3340]])' },
        { code: '' },
        { code: 'Part 6: The Backward Pass - Calculating Gradients' },
        { code: '' },
        { code: 'If the forward pass was the model\'s "guess," the backward pass is the "post-mortem analysis." We compare the guess to the truth, calculate how wrong we were, and then determine exactly how to change each parameter to be less wrong next time.' },
        { code: '' },
        { code: '6.1. Defining Error: The Loss Function' },
        { code: 'We need a single number that tells us how "wrong" our predictions are. This is called the Loss. For regression, the most common loss function is the Mean Squared Error (MSE).' },
        { code: '' },
        { code: '# y_hat is our prediction from the forward pass' },
        { code: '# y_true is the ground truth' },
        { code: '# Let\'s calculate the loss manually' },
        { code: 'error = y_hat - y_true' },
        { code: 'squared_error = error ** 2' },
        { code: 'loss = squared_error.mean()' },
        { code: '' },
        { code: 'print(f"Prediction (first 3):\\n {y_hat[:3]}\\n")' },
        { code: 'print(f"Truth (first 3):\\n {y_true[:3]}\\n")' },
        { code: 'print(f"Loss (a single number): {loss}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Prediction (first 3):' },
        { code: ' tensor([[ 0.0737],' },
        { code: '        [ 0.1812],' },
        { code: '        [ 0.1485]], grad_fn=<SliceBackward0>)' },
        { code: '' },
        { code: 'Truth (first 3):' },
        { code: ' tensor([[-0.1030],' },
        { code: '        [ 0.4491],' },
        { code: '        [ 0.3340]])' },
        { code: '' },
        { code: 'Loss (a single number): 1.6322047710418701' },
        { code: '' },
        { code: '6.2. The Magic Command: loss.backward()' },
        { code: 'This is where the magic of Autograd happens. With a single command, we tell PyTorch to send a signal backward from the loss through the entire computation graph it built during the forward pass.' },
        { code: '' },
        { code: '# The backward pass' },
        { code: 'loss.backward()' },
        { code: '' },
        { code: '6.3. Inspecting the Result: The .grad Attribute' },
        { code: 'The .grad attribute now holds the gradient for each parameter. This is the "signal" that tells us how to adjust our knobs.' },
        { code: '' },
        { code: '# The gradients are now stored in the .grad attribute of our parameters' },
        { code: 'print(f"Gradient for W (dL/dW):\\n {W.grad}\\n")' },
        { code: 'print(f"Gradient for b (dL/db):\\n {b.grad}")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Gradient for W (dL/dW):' },
        { code: ' tensor([[-1.0185]])' },
        { code: '' },
        { code: 'Gradient for b (dL/db):' },
        { code: ' tensor([-2.0673])' },
        { code: '' },
        { code: 'Part 7: The Training Loop - Gradient Descent From Scratch' },
        { code: '' },
        { code: 'This is the heart of the entire deep learning process. The Training Loop repeatedly executes the forward and backward passes, incrementally updating the model\'s parameters to minimize the loss.' },
        { code: '' },
        { code: '7.1. The Algorithm: Gradient Descent' },
        { code: 'The core update rule for gradient descent was promised in the very beginning, and now we can finally implement it: theta_new = theta_old - learning_rate * gradient' },
        { code: '' },
        { code: '7.2. The Loop: Putting It All Together' },
        { code: '' },
        { code: '# Hyperparameters' },
        { code: 'learning_rate = 0.01' },
        { code: 'epochs = 100' },
        { code: '' },
        { code: '# Let\'s re-initialize our random parameters' },
        { code: 'W = torch.randn(1, 1, requires_grad=True)' },
        { code: 'b = torch.randn(1, requires_grad=True)' },
        { code: '' },
        { code: 'print(f"Starting Parameters: W={W.item():.3f}, b={b.item():.3f}\\n")' },
        { code: '' },
        { code: '# The Training Loop' },
        { code: 'for epoch in range(epochs):' },
        { code: '    ### STEP 1 & 2: Forward Pass and Loss Calculation ###' },
        { code: '    y_hat = X @ W + b' },
        { code: '    loss = torch.mean((y_hat - y_true)**2)' },
        { code: '' },
        { code: '    ### STEP 3: Backward Pass (Calculate Gradients) ###' },
        { code: '    loss.backward()' },
        { code: '' },
        { code: '    ### STEP 4: Update Parameters (The Gradient Descent Step) ###' },
        { code: '    # We wrap this in no_grad() because this is not part of the model\'s computation' },
        { code: '    with torch.no_grad():' },
        { code: '        W -= learning_rate * W.grad' },
        { code: '        b -= learning_rate * b.grad' },
        { code: '' },
        { code: '    ### STEP 5: Zero the Gradients ###' },
        { code: '    # We must reset the gradients for the next iteration' },
        { code: '    W.grad.zero_()' },
        { code: '    b.grad.zero_()' },
        { code: '' },
        { code: '    # Optional: Print progress' },
        { code: '    if epoch % 10 == 0:' },
        { code: '        print(f"Epoch {epoch:02d}: Loss={loss.item():.4f}, W={W.item():.3f}, b={b.item():.3f}")' },
        { code: '' },
        { code: 'print(f"\\nFinal Parameters: W={W.item():.3f}, b={b.item():.3f}")' },
        { code: 'print(f"True Parameters:  W=2.000, b=1.000")' },
        { code: '' },
        { code: 'Output:' },
        { code: 'Starting Parameters: W=-0.369, b=0.485' },
        { code: '' },
        { code: 'Epoch 00: Loss=4.1451, W=-0.347, b=0.505' },
        { code: '...' },
        { code: 'Epoch 90: Loss=0.0329, W=1.711, b=1.127' },
        { code: '' },
        { code: 'Final Parameters: W=1.731, b=1.115' },
        { code: 'True Parameters:  W=2.000, b=1.000' },
        { code: '' },
        { code: 'Part 8: Professional Building Blocks - torch.nn Layers' },
        { code: '' },
        { code: 'Manually creating and managing weights (W) and biases (b) is great for understanding the fundamentals, but it doesn\'t scale. This is where the torch.nn module comes in.' },
        { code: '' },
        { code: '8.1. The Workhorse: torch.nn.Linear' },
        { code: 'The torch.nn.Linear layer does exactly what our manual X @ W + b operation did.' },
        { code: '' },
        { code: '# The input to our model has 1 feature (D_in=1)' },
        { code: '# The output of our model is 1 value (D_out=1)' },
        { code: 'D_in = 1' },
        { code: 'D_out = 1' },
        { code: '' },
        { code: '# Create a Linear layer' },
        { code: 'linear_layer = torch.nn.Linear(in_features=D_in, out_features=D_out)' },
        { code: '' },
        { code: '# You can inspect the randomly initialized parameters inside' },
        { code: 'print(f"Layer\'s Weight (W): {linear_layer.weight}\\n")' },
        { code: 'print(f"Layer\'s Bias (b): {linear_layer.bias}\\n")' },
        { code: '' },
        { code: '# You use it just like a function. Let\'s pass our data X through it.' },
        { code: 'y_hat_nn = linear_layer(X)' },
        { code: '' },
        { code: 'print(f"Output of nn.Linear (first 3 rows):\\n {y_hat_nn[:3]}")' },
        { code: '' },
        { code: '8.2. Introducing Non-Linearity: Activation Functions' },
        { code: 'To learn complex, real-world patterns, neural networks need to introduce "kinks" or non-linearities between these linear layers. This is the job of an activation function.' },
        { code: '' },
        { code: 'nn.ReLU (Rectified Linear Unit)' },
        { code: 'Formula: ReLU(x) = max(0, x)' },
        { code: '' },
        { code: 'relu = torch.nn.ReLU()' },
        { code: 'sample_data = torch.tensor([-2.0, -0.5, 0.0, 0.5, 2.0])' },
        { code: 'activated_data = relu(sample_data)' },
        { code: '' },
        { code: 'print(f"Original Data:      {sample_data}")' },
        { code: 'print(f"Data after ReLU:    {activated_data}")' },
        { code: '' },
        { code: 'nn.GELU (Gaussian Error Linear Unit)' },
        { code: 'This is the modern standard for high-performance models, especially Transformers like BERT and GPT.' },
        { code: '' },
        { code: 'gelu = torch.nn.GELU()' },
        { code: 'activated_data = gelu(sample_data)' },
        { code: 'print(f"Data after GELU:    {activated_data}")' },
        { code: '' },
        { code: 'nn.Softmax' },
        { code: 'Softmax is a special activation function used almost exclusively on the final output layer of a classification model. Its job is to take a vector of raw, unbounded scores (called logits) and convert them into a probability distribution.' },
        { code: '' },
        { code: 'softmax = torch.nn.Softmax(dim=-1)' },
        { code: 'logits = torch.tensor([[1.0, 3.0, 0.5, 1.5], [-1.0, 2.0, 1.0, 0.0]])' },
        { code: 'probabilities = softmax(logits)' },
        { code: 'print(f"Output Probabilities:\\n {probabilities}\\n")' },
        { code: '' },
        { code: '8.3. Essential Layers for LLMs' },
        { code: '' },
        { code: 'nn.Embedding' },
        { code: 'An Embedding layer is the bridge between human language and the model\'s internal world of vectors. It acts as a learnable lookup table.' },
        { code: '' },
        { code: 'vocab_size = 10' },
        { code: 'embedding_dim = 3' },
        { code: 'embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)' },
        { code: 'input_ids = torch.tensor([[1, 5, 0, 8]])' },
        { code: 'word_vectors = embedding_layer(input_ids)' },
        { code: '' },
        { code: 'nn.LayerNorm (Layer Normalization)' },
        { code: 'Layer Normalization stabilizes training by re-centering and re-scaling the activations for each individual data sample.' },
        { code: '' },
        { code: 'feature_dim = 3' },
        { code: 'norm_layer = torch.nn.LayerNorm(normalized_shape=feature_dim)' },
        { code: 'input_features = torch.tensor([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]])' },
        { code: 'normalized_features = norm_layer(input_features)' },
        { code: '' },
        { code: 'nn.Dropout' },
        { code: 'Dropout is a simple but remarkably effective regularization technique to prevent overfitting. During training, it randomly sets a fraction of the input tensor\'s elements to zero.' },
        { code: '' },
        { code: 'dropout_layer = torch.nn.Dropout(p=0.5)' },
        { code: 'input_tensor = torch.ones(1, 10)' },
        { code: 'dropout_layer.train() # Activate dropout' },
        { code: 'output_during_train = dropout_layer(input_tensor)' },
        { code: '' },
        { code: 'Part 9: Assembling Models & The Professional Training Loop' },
        { code: '' },
        { code: '9.1. The Model Blueprint: class MyModel(nn.Module)' },
        { code: 'Every PyTorch model is a Python class that inherits from torch.nn.Module. You only need to define two special methods: __init__ and forward.' },
        { code: '' },
        { code: 'import torch.nn as nn' },
        { code: '' },
        { code: 'class LinearRegressionModel(nn.Module):' },
        { code: '    def __init__(self, in_features, out_features):' },
        { code: '        super().__init__()' },
        { code: '        self.linear_layer = nn.Linear(in_features, out_features)' },
        { code: '' },
        { code: '    def forward(self, x):' },
        { code: '        return self.linear_layer(x)' },
        { code: '' },
        { code: 'model = LinearRegressionModel(in_features=1, out_features=1)' },
        { code: '' },
        { code: '9.2. The Optimizer: torch.optim' },
        { code: 'Next, we replace our manual weight update step with an optimizer from the torch.optim library.' },
        { code: '' },
        { code: 'import torch.optim as optim' },
        { code: '' },
        { code: 'learning_rate = 0.01' },
        { code: 'optimizer = optim.Adam(model.parameters(), lr=learning_rate)' },
        { code: 'loss_fn = nn.MSELoss() # Mean Squared Error Loss' },
        { code: '' },
        { code: '9.3. The Final, Clean Training Loop' },
        { code: 'Now we can rewrite our training loop. The five manual steps are replaced by three elegant, high-level commands.' },
        { code: '' },
        { code: 'The Three-Line Mantra:' },
        { code: '1. optimizer.zero_grad()' },
        { code: '2. loss.backward()' },
        { code: '3. optimizer.step()' },
        { code: '' },
        { code: 'epochs = 100' },
        { code: '' },
        { code: 'for epoch in range(epochs):' },
        { code: '    ### FORWARD PASS ###' },
        { code: '    y_hat = model(X)' },
        { code: '' },
        { code: '    ### CALCULATE LOSS ###' },
        { code: '    loss = loss_fn(y_hat, y_true)' },
        { code: '' },
        { code: '    ### THE THREE-LINE MANTRA ###' },
        { code: '    optimizer.zero_grad()' },
        { code: '    loss.backward()' },
        { code: '    optimizer.step()' },
        { code: '' },
        { code: 'Part 10: The Big Picture - From Our Model to an LLM' },
        { code: '' },
        { code: 'You might be thinking, "This is great for a toy linear regression model, but how does this relate to a massive, complex Large Language Model like GPT or Llama?"' },
        { code: '' },
        { code: 'The answer is simple: It\'s not an analogy. You have learned the exact, fundamental components and the universal process used to train them.' },
        { code: '' },
        { code: '10.1. The Direct Link: The Transformer\'s Feed-Forward Network' },
        { code: 'Every block in a Transformer contains a sub-component called a Feed-Forward Network (FFN). It\'s just a simple two-layer Multi-Layer Perceptron (MLP). You already have all the knowledge to build one.' },
        { code: '' },
        { code: 'import torch.nn as nn' },
        { code: '' },
        { code: 'class FeedForwardNetwork(nn.Module):' },
        { code: '    def __init__(self, embedding_dim, ffn_dim):' },
        { code: '        super().__init__()' },
        { code: '        self.layer1 = nn.Linear(embedding_dim, ffn_dim)' },
        { code: '        self.activation = nn.GELU()' },
        { code: '        self.layer2 = nn.Linear(ffn_dim, embedding_dim)' },
        { code: '' },
        { code: '    def forward(self, x):' },
        { code: '        x = self.layer1(x)' },
        { code: '        x = self.activation(x)' },
        { code: '        x = self.layer2(x)' },
        { code: '        return x' },
        { code: '' },
        { code: 'The FFN inside a multi-billion parameter model is literally this simple.' },
        { code: '' },
        { code: '10.3. The Universal Truth of Training' },
        { code: 'Most importantly, the process we used to train our 2-parameter model is the exact same process used to train a multi-billion parameter LLM.' },
        { code: '' },
        { code: '1. Forward Pass: y_hat = model(X)' },
        { code: '2. Calculate Loss: loss = loss_fn(y_hat, y_true)' },
        { code: '3. Zero Gradients: optimizer.zero_grad()' },
        { code: '4. Backward Pass: loss.backward()' },
        { code: '5. Update Parameters: optimizer.step()' },
        { code: '' },
        { code: 'You\'ve Done It.' },
        { code: 'In this hour, we have journeyed from the most basic element—a single tensor—to understanding the engine that powers the largest and most complex AI models in the world.' },
      ],
    },
  };