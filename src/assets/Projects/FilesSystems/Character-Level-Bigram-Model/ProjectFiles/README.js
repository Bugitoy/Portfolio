export const READMEFile = {
    'README.md': {
      type: 'file',
      title: 'README.md',
      lines: [
        { code: 'bigram.py â€“ Minimal Bigram Character Model' },
        { code: '' },
        { code: 'What it does' },
        { code: '- Trains a tiny character-level bigram language model on input.txt.' },
        { code: '- Learns next-character probabilities that depend only on the current character (no long-range context).' },
        { code: '- After training, samples text autoregressively.' },
        { code: '' },
        { code: 'Key components' },
        { code: '- Data prep: builds a character vocabulary from input.txt, defines encode/decode, splits 90%/10% into train/val.' },
        { code: '- Model: BigramLanguageModel with a single nn.Embedding(vocab_size, vocab_size) producing next-token logits directly.' },
        { code: '- Training: random contiguous blocks from the corpus (block_size), cross-entropy loss, AdamW optimizer.' },
        { code: '- Generation: starts from a zero token and repeatedly samples the next character via softmax.' },
        { code: '' },
        { code: 'Important hyperparameters (defaults in file)' },
        { code: '- batch_size=32, block_size=8, learning_rate=1e-2, max_iters=3000.' },
        { code: '' },
        { code: 'How to run' },
        { code: 'python bigram.py' },
        { code: '' },
        { code: 'Expected output: periodic train/val losses and a printed sample at the end.' },
        { code: '' },
        { code: 'Notes' },
        { code: '- This model captures unigram/bigram statistics only; it cannot model long-range structure.' },
        { code: '- Uses GPU automatically if available (device = \'cuda\' if torch.cuda.is_available() else \'cpu\').' },
      ],
    },
  };