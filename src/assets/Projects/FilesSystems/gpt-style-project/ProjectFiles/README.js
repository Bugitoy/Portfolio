export const gptReadmeFile = {
  'README.md': {
    type: 'file',
    title: 'README.md',
    lines: [
      { code: 'gpt.py – Small GPT-Style Character Transformer' },
      { code: '' },
      { code: 'What it does' },
      { code: '- Trains a compact GPT-like Transformer on input.txt at the character level.' },
      { code: '- Learns long-range dependencies up to block_size using masked self-attention.' },
      { code: '- Samples text autoregressively after training; optionally can dump long samples to more.txt.' },
      { code: '' },
      { code: 'Architecture overview' },
      { code: '- Token/position embeddings: nn.Embedding(vocab_size, n_embd) and nn.Embedding(block_size, n_embd).' },
      { code: '- Transformer blocks: n_layer stacks of pre-norm blocks with residuals.' },
      { code: '  - MultiHeadAttention: parallel Heads, each with key/query/value projections and a causal mask.' },
      { code: '  - FeedFoward: 2-layer MLP with expansion 4 * n_embd and dropout.' },
      { code: '- Final LayerNorm and linear head lm_head to logits over the vocabulary.' },
      { code: '- Custom _init_weights for stable training.' },
      { code: '' },
      { code: 'Important hyperparameters (defaults in file)' },
      { code: '- batch_size=64, block_size=256, n_embd=384, n_head=6, n_layer=6, dropout=0.2, learning_rate=3e-4, max_iters=5000.' },
      { code: '' },
      { code: 'Data & batching' },
      { code: '' },
      { code: '- Builds a char vocabulary from input.txt; 90%/10% train/val split.' },
      { code: '- get_batch draws random contiguous sequences of length block_size and their next-char targets.' },
      { code: '' },
      { code: 'Training loop' },
      { code: '- Optimizer: AdamW.' },
      { code: '- Periodic evaluation on train/val via estimate_loss().' },
      { code: '- Prints parameter count at startup.' },
      { code: '' },
      { code: 'Generation' },
      { code: '- Crops the running context to the last block_size tokens and samples next chars via softmax.' },
      { code: '- End of script prints a short sample; a longer sample write to more.txt is available but commented.' },
      { code: '' },
      { code: 'How to run' },
      { code: 'python gpt.py' },
      { code: '' },
      { code: 'Expected output: periodic train/val losses, parameter count, and a printed sample at the end.' },
      { code: '' },
      { code: 'Notes' },
      { code: '- Automatically uses GPU if available.' },
      { code: '- For larger corpora or longer training, consider saving/loading checkpoints (not included in this minimal script).' },
      { code: '' },
      { code: '' },
      { code: 'Example training log and sample output' },
      { code: '' },
      { code: 'The following run used the defaults in gpt.py and tiny Shakespeare (input.txt). It shows typical convergence and a representative generated sample after training:' },
      { code: '' },
      { code: '0.209729 M parameters' },
      { code: 'step 0: train loss 4.4116, val loss 4.4022' },
      { code: 'step 100: train loss 2.6568, val loss 2.6670' },
      { code: 'step 200: train loss 2.5090, val loss 2.5058' },
      { code: 'step 300: train loss 2.4198, val loss 2.4340' },
      { code: 'step 400: train loss 2.3503, val loss 2.3567' },
      { code: 'step 500: train loss 2.2970, val loss 2.3136' },
      { code: 'step 600: train loss 2.2410, val loss 2.2506' },
      { code: 'step 700: train loss 2.2062, val loss 2.2198' },
      { code: 'step 800: train loss 2.1638, val loss 2.1871' },
      { code: 'step 900: train loss 2.1232, val loss 2.1494' },
      { code: 'step 1000: train loss 2.1020, val loss 2.1293' },
      { code: 'step 1100: train loss 2.0704, val loss 2.1196' },
      { code: 'step 1200: train loss 2.0382, val loss 2.0798' },
      { code: 'step 1300: train loss 2.0249, val loss 2.0640' },
      { code: 'step 1400: train loss 1.9922, val loss 2.0354' },
      { code: 'step 1500: train loss 1.9707, val loss 2.0308' },
      { code: 'step 1600: train loss 1.9614, val loss 2.0474' },
      { code: 'step 1700: train loss 1.9393, val loss 2.0130' },
      { code: 'step 1800: train loss 1.9070, val loss 1.9943' },
      { code: 'step 1900: train loss 1.9057, val loss 1.9871' },
      { code: 'step 2000: train loss 1.8834, val loss 1.9954' },
      { code: 'step 2100: train loss 1.8719, val loss 1.9758' },
      { code: 'step 2200: train loss 1.8582, val loss 1.9623' },
      { code: 'step 2300: train loss 1.8546, val loss 1.9517' },
      { code: 'step 2400: train loss 1.8410, val loss 1.9476' },
      { code: 'step 2500: train loss 1.8167, val loss 1.9455' },
      { code: 'step 2600: train loss 1.8263, val loss 1.9401' },
      { code: 'step 2700: train loss 1.8108, val loss 1.9340' },
      { code: 'step 2800: train loss 1.8040, val loss 1.9247' },
      { code: 'step 2900: train loss 1.8044, val loss 1.9304' },
      { code: 'step 3000: train loss 1.7963, val loss 1.9242' },
      { code: 'step 3100: train loss 1.7687, val loss 1.9147' },
      { code: 'step 3200: train loss 1.7547, val loss 1.9102' },
      { code: 'step 3300: train loss 1.7557, val loss 1.9037' },
      { code: 'step 3400: train loss 1.7547, val loss 1.8946' },
      { code: 'step 3500: train loss 1.7385, val loss 1.8968' },
      { code: 'step 3600: train loss 1.7260, val loss 1.8914' },
      { code: 'step 3700: train loss 1.7257, val loss 1.8808' },
      { code: 'step 3800: train loss 1.7204, val loss 1.8919' },
      { code: 'step 3900: train loss 1.7215, val loss 1.8788' },
      { code: 'step 4000: train loss 1.7146, val loss 1.8639' },
      { code: 'step 4100: train loss 1.7095, val loss 1.8724' },
      { code: 'step 4200: train loss 1.7079, val loss 1.8707' },
      { code: 'step 4300: train loss 1.7035, val loss 1.8502' },
      { code: 'step 4400: train loss 1.7043, val loss 1.8693' },
      { code: 'step 4500: train loss 1.6914, val loss 1.8522' },
      { code: 'step 4600: train loss 1.6853, val loss 1.8357' },
      { code: 'step 4700: train loss 1.6862, val loss 1.8483' },
      { code: 'step 4800: train loss 1.6671, val loss 1.8434' },
      { code: 'step 4900: train loss 1.6736, val loss 1.8415' },
      { code: 'step 4999: train loss 1.6635, val loss 1.8226' },
      { code: '' },
      { code: 'FlY BOLINGLO:' },
      { code: 'Them thrumply towiter arts the' },
      { code: 'muscue rike begatt the sea it' },
      { code: 'What satell in rowers that some than othis Marrity.' },
      { code: '' },
      { code: 'LUCENTVO:' },
      { code: 'But userman these that, where can is not diesty rege;' },
      { code: 'What and see to not. But\'s eyes. What?' },
      { code: '' },
      { code: 'JOHN MARGARET:' },
      { code: 'Than up I wark, what out, I ever of and love,' },
      { code: 'one these do sponce, vois I me;' },
      { code: 'But my pray sape to ries all to the not erralied in may.' },
      { code: '' },
      { code: 'BENVOLIO:' },
      { code: 'To spits as stold\'s bewear I would and say mesby all' },
      { code: 'on sworn make he anough' },
      { code: 'As cousins the solle, whose be my conforeful may lie them yet' },
      { code: 'nobe allimely untraled to be thre I say be,' },
      { code: 'Notham a brotes theme an make come,' },
      { code: 'And that his reach to the duke ento' },
      { code: 'the grmeants bell! and now there king-liff-or grief?' },
      { code: '' },
      { code: 'GLOUCESTER:' },
      { code: 'All the bettle dreene, for To his like thou thron!' },
      { code: '' },
      { code: 'MENENIUS:' },
      { code: 'Then, if I knom her all.' },
      { code: 'My lord, but terruly friend' },
      { code: 'Rish of the ploceiness and wilt tends sure?' },
      { code: 'Is you knows a fasir wead' },
      { code: 'That with him my spaut,' },
      { code: 'I shall not tas where\'s not, becomity; my coulds sting,' },
      { code: 'then the wit be dong to tyget our hereefore,' },
      { code: 'Who strop me, mend here, if agains, bitten, thy lack.' },
      { code: 'The but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-' },
      { code: 'How the enclady: It you and how,' },
      { code: 'I am in him, And ladderle:' },
      { code: 'Their hand whose wife, it my hithre,' },
      { code: 'Roman and where sposs gives\'d you.' },
      { code: '' },
      { code: 'TROMIOLANUS:' },
      { code: 'But livants you great, I shom mistrot come, for to she to lot' },
      { code: 'for smy to men ventry mehus. Gazise;' },
      { code: 'Full\'t were some the cause, and stouch set,' },
      { code: 'Or promises, which a kingsasted to your gove them; and sterrer,' },
      { code: 'And that wae love him.' },
      { code: '' },
      { code: 'BRUTUS:' },
      { code: 'You shape with these sweet.' },
      { code: '' },
      { code: 'CORTENGONO:' },
      { code: 'Lo, where \'twon elmes, \'morth young agres;' },
      { code: 'Sir, azavoust to striel accurded we missery sets crave.' },
      { code: '' },
      { code: 'ANGOLUM:' },
      { code: 'For is Henry to have gleise the dreason' },
      { code: 'That I ant shorfold wefth their servy in enscy.' },
      { code: '' },
      { code: 'ISABELLA:' },
      { code: 'O, I better you eyse such formfetrews.' },
      { code: '' },
      { code: 'BUCKINGHARENT:' },
      { code: 'Qead my lightle this righanneds flase them' },
      { code: 'Wam which an take was our some pleasurs,' },
      { code: 'Lovisoname to me, then fult me?--have it?' },
      { code: '' },
      { code: 'HENRY BOLINGBROY:' },
      { code: 'That wha' },
      { code: '' },
      { code: 'Interpreting the metrics' },
      { code: '' },
      { code: '- step N: Training iteration index; each step = forward + backward + optimizer update.' },
      { code: '- train loss: Average cross-entropy on sampled training batches (lower is better).' },
      { code: '- val loss: Average cross-entropy on held-out validation batches (tracks generalization).' },
      { code: '- 0.209729 M parameters (example): Total trainable parameters in millions.' },
      { code: '- Tip: Convert loss to perplexity as perplexity = exp(loss); e.g., val loss 1.82 ≈ perplexity 6.17.' },
      { code: '' },
      { code: 'Key figures and configuration' },
      { code: '' },
      { code: '- Vocabulary size (|V|): number of unique characters in input.txt (computed at runtime as len(set(text))). With tiny Shakespeare it is typically ~65.' },
      { code: '- Context length: block_size = 256 (max characters the model conditions on).' },
      { code: '- Batch size: batch_size = 64 sequences per optimization step.' },
      { code: '- Tokens per step: batch_size × block_size = 16,384 characters/step.' },
      { code: '- Model size: ~0.210M parameters with defaults (printed at startup).' },
      { code: '- Embedding dimension: n_embd = 384.' },
      { code: '- Attention heads: n_head = 6 (head size = n_embd / n_head = 64).' },
      { code: '- Transformer layers: n_layer = 6.' },
      { code: '- Dropout: dropout = 0.2.' },
      { code: '- Learning rate / Optimizer: learning_rate = 3e-4, AdamW.' },
      { code: '- Train/Val split: 90% / 10% of the character stream.' },
      { code: '- Eval cadence: eval_interval = 500, eval_iters = 200.' },
      { code: '- Device: uses CUDA if available, else CPU.' },
    ],
  },
};